{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72b24e3e",
   "metadata": {
    "id": "72b24e3e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a084c2d",
   "metadata": {
    "id": "5a084c2d"
   },
   "outputs": [],
   "source": [
    "# Directory containing the \"armFlapping\" videos\n",
    "# video_dir = r'C:\\Users\\arjun\\Desktop\\College\\project\\SSBD data\\armFlapping'\n",
    "video_dir = r\"C:\\Users\\arjun\\Desktop\\College\\project\\SSBD data\\ssbd2\\armFlapping\\armFlapping\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "P_xOdKJdeLel",
   "metadata": {
    "id": "P_xOdKJdeLel"
   },
   "outputs": [],
   "source": [
    "# COMMENT_SR: this cell can be removed\n",
    "# !kaggle datasets download -d shradheypathak/ssbd3\n",
    "# !unzip ssbd3.zip\n",
    "# !pwd\n",
    "# !cd ssbd2/armFlapping/armFlapping && idx=1 && IFS=$'\\n'; for i in `ls`; do mv $i $idx.avi; idx=$(($idx + 1)); done\n",
    "# !ls ssbd2/armFlapping/armFlapping\n",
    "# video_dir = r'/content/ssbd2/armFlapping/armFlapping'\n",
    "\n",
    "# !apt install mediainfo\n",
    "# !mediainfo video_file.mp4 /content/ssbd2/armFlapping/armFlapping/1.avi\n",
    "# !ffmpeg -codecs | grep -i avc\n",
    "\n",
    "# video_path = str(video_f).replace(\".avi\", \".mp4\")\n",
    "# os.popen(\"ffmpeg -i '{input}' -c:v copy -c:a copy -f mp4 '{output}'\".format(input = video_f, output = video_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9dcc2ed",
   "metadata": {
    "id": "c9dcc2ed"
   },
   "outputs": [],
   "source": [
    "# Specify the number of classes and videos per class\n",
    "NUM_CLASSES = 2  # Only one class for armFlapping\n",
    "VIDEOS_PER_CLASS = 56  # Total number of videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c3c9c8e",
   "metadata": {
    "id": "5c3c9c8e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install scikit-video\n",
    "# import skvideo.io\n",
    "import os\n",
    "\n",
    "def frames_from_video_directory(video_dir, n_frames, output_size=(224, 224), frame_step=15):\n",
    "    \"\"\"\n",
    "    Creates frames from each video file in the specified directory.\n",
    "\n",
    "    Args:\n",
    "      video_dir: Directory containing video files.\n",
    "      n_frames: Number of frames to be created per video file.\n",
    "      output_size: Pixel size of the output frame image.\n",
    "      frame_step: Number of frames to skip before selecting the next frame.\n",
    "\n",
    "    Return:\n",
    "      A NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "\n",
    "      # COMMENT_SR:\n",
    "      - Either the model or this function required changes for it to work.\n",
    "      - For now, have modified this function to output result in the shape of\n",
    "        (n_videos, n_frames, height, width, channels), since that seemed more appropriate to me.\n",
    "      - In case this needs to be in the original format, the model has to be changed accordingly.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "\n",
    "    # List all video files in the directory\n",
    "    video_files = [os.path.join(video_dir, f'{i}.mp4') for i in range(1, VIDEOS_PER_CLASS + 1)]\n",
    "\n",
    "    for video_path in video_files:\n",
    "        # COMMENT_SR: added this for the extra dimension (refer to the docstring)\n",
    "        video_frames = []\n",
    "\n",
    "        # Open the video file\n",
    "        src = cv2.VideoCapture(video_path)\n",
    "\n",
    "        if not src.isOpened():\n",
    "            print(f\"Warning: Unable to open video file: {video_path}\")\n",
    "            continue  # Skip to the next video if unable to open\n",
    "\n",
    "        video_length = int(src.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "        if need_length > video_length:\n",
    "            start = 0\n",
    "        else:\n",
    "            max_start = video_length - need_length\n",
    "            start = random.randint(0, max_start + 1)\n",
    "\n",
    "        src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "        ret, frame = src.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(f\"Unable to read frame for {video_path}\")\n",
    "            continue  \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        video_frames.append(format_frames(frame, output_size))\n",
    "\n",
    "        for _ in range(n_frames - 1):\n",
    "            for _ in range(frame_step):\n",
    "                ret, frame = src.read()\n",
    "            ret, frame = src.read()\n",
    "            if not ret:\n",
    "                print(f\"Unable to read frame for {video_path}\")\n",
    "                frame = np.zeros_like(result[0])\n",
    "#                 continue\n",
    "            else:\n",
    "#                 print(f\"{video_path}\")\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                frame = format_frames(frame, output_size)\n",
    "                video_frames.append(frame)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        src.release()\n",
    "        result.append(video_frames)\n",
    "\n",
    "    result = np.array(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2b5c4b",
   "metadata": {
    "id": "4b2b5c4b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe35f386",
   "metadata": {
    "id": "fe35f386"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9e89586",
   "metadata": {
    "id": "a9e89586"
   },
   "outputs": [],
   "source": [
    "def format_frames(frame, output_size):\n",
    "    \"\"\"\n",
    "    Pad and resize an image from a video.\n",
    "\n",
    "    Args:\n",
    "      frame: Image that needs to be resized and padded.\n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      Formatted frame with padding of the specified output size.\n",
    "    \"\"\"\n",
    "    frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "    frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57fb3433",
   "metadata": {
    "id": "57fb3433"
   },
   "outputs": [],
   "source": [
    "# from google.colab.patches import cv2_imshow\n",
    "# Function to display frames\n",
    "def display_frames(frames):\n",
    "    for video in frames:\n",
    "        for frame in video:\n",
    "            # cv2.imshow(\"Video Frame\", frame)\n",
    "            # cv2_imshow(frame)\n",
    "            # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            #     break\n",
    "\n",
    "            # COMMENT_SR: using matplotlib instead of opencv\n",
    "            # because of some known codec issues\n",
    "            from matplotlib import pyplot as plt\n",
    "            plt.figure()\n",
    "            plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "YZEsfBkXOh7f",
   "metadata": {
    "id": "YZEsfBkXOh7f"
   },
   "outputs": [],
   "source": [
    "# dire = r\"C:\\Users\\arjun\\Desktop\\College\\project\\SSBD data\\ssbd2\\armFlapping\\armFlapping\"\n",
    "# import os\n",
    "# for i,f in enumerate(os.listdir(dire)):\n",
    "#     print(f)\n",
    "#     f1=os.fsdecode(f)\n",
    "#     os.rename(\"%s\\%s\"%(dire,f1), \"%s\\%s.mp4\"%(dire,i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8acfa234",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8acfa234",
    "outputId": "b2eb3da4-c4cd-4bda-c4e6-60e844e0e8f4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to read frame for C:\\Users\\arjun\\Desktop\\College\\project\\SSBD data\\ssbd2\\armFlapping\\armFlapping\\42.mp4\n"
     ]
    }
   ],
   "source": [
    "# Define the number of frames to extract per video\n",
    "n_frames_per_video = 3  # Set this to your desired number of frames\n",
    "\n",
    "# Extract frames and display\n",
    "frames = frames_from_video_directory(video_dir, n_frames_per_video)\n",
    "# display_frames(frames)\n",
    "\n",
    "# Close the OpenCV window after displaying frames\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ac8b958",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ac8b958",
    "outputId": "7ca6607e-88c1-4439-8728-5e5f233bdbab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of frames array: (56, 3, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the frames array\n",
    "if frames is not None:\n",
    "    print(\"Shape of frames array:\", frames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b474732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a42595d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a42595d",
    "outputId": "9b825ba2-791f-49f9-c3be-5a07d045af85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 3, 224, 224, 3)   0         \n",
      "                             ]                                   \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 3, 1280)           2257984   \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                344320    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2602369 (9.93 MB)\n",
      "Trainable params: 344385 (1.31 MB)\n",
      "Non-trainable params: 2257984 (8.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define your MobileNetV2-based model\n",
    "def create_mobilenetv2_model(input_shape):\n",
    "    base_model = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=input_shape, include_top=False, weights='imagenet'\n",
    "    )\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False  # Freeze the MobileNetV2 layers\n",
    "\n",
    "    x = base_model.output\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    return tf.keras.models.Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Define a model combining MobileNetV2 and LSTM\n",
    "def create_mobilenetv2_lstm_model(input_shape, n_frames, lstm_units):\n",
    "    mobilenetv2_model = create_mobilenetv2_model(input_shape)\n",
    "\n",
    "    # COMMENT_SR: this layer needs to change in case the original dimension\n",
    "    # has to be preserved (refer to docstring of frames_from_video_directory)\n",
    "    frame_input = tf.keras.layers.Input(shape=(n_frames, *input_shape))\n",
    "\n",
    "    frames = tf.keras.layers.TimeDistributed(mobilenetv2_model)(frame_input)\n",
    "\n",
    "    lstm_output = tf.keras.layers.LSTM(lstm_units)(frames)\n",
    "\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(lstm_output)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=frame_input, outputs=output)\n",
    "    return model\n",
    "\n",
    "# Define the input shape based on the output_size from your frame extraction code\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "# Number of LSTM units\n",
    "lstm_units = 64\n",
    "\n",
    "# Create the model\n",
    "model = create_mobilenetv2_lstm_model(input_shape, n_frames_per_video, lstm_units)\n",
    "\n",
    "# Compile the model with appropriate loss and optimizer\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',  # Use appropriate loss function\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),  # Adjust the learning rate\n",
    "    metrics=['accuracy']  # Add more metrics if needed\n",
    ")\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# You can now train the model using your video frames data\n",
    "# Make sure to have your data and labels ready for training\n",
    "# model.fit(...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1840ed81",
   "metadata": {
    "id": "1840ed81"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eab2939",
   "metadata": {
    "id": "9eab2939"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load your video frames and labels\n",
    "X = frames  # Replace 'frames' with your actual video frames\n",
    "y = np.array([0] * VIDEOS_PER_CLASS)  # Replace with your actual labels\n",
    "\n",
    "# Normalize the frame data (you may need to adjust the normalization method)\n",
    "X = X / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baOpM9eaU7WI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "baOpM9eaU7WI",
    "outputId": "57c05ea3-3d19-423f-d1cf-426775cea7a6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c1884c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 3, 224, 224, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67a457ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67a457ef",
    "outputId": "2f57812c-4187-437d-f502-20c0f3fb0aba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 10s 4s/step - loss: nan - accuracy: 0.5909 - val_loss: nan - val_accuracy: 0.9167\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 5s 2s/step - loss: nan - accuracy: 0.8409 - val_loss: nan - val_accuracy: 0.9167\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 5s 2s/step - loss: nan - accuracy: 0.8409 - val_loss: nan - val_accuracy: 0.9167\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 5s 2s/step - loss: nan - accuracy: 0.8409 - val_loss: nan - val_accuracy: 0.9167\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 5s 2s/step - loss: nan - accuracy: 0.8409 - val_loss: nan - val_accuracy: 0.9167\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 5s 2s/step - loss: nan - accuracy: 0.8409 - val_loss: nan - val_accuracy: 0.9167\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 5s 2s/step - loss: nan - accuracy: 0.8409 - val_loss: nan - val_accuracy: 0.9167\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 5s 2s/step - loss: nan - accuracy: 0.8409 - val_loss: nan - val_accuracy: 0.9167\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 5s 3s/step - loss: nan - accuracy: 0.8409 - val_loss: nan - val_accuracy: 0.9167\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 5s 2s/step - loss: nan - accuracy: 0.8409 - val_loss: nan - val_accuracy: 0.9167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a3a16a0b20>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split your data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define your video labels, where 0 represents one class and 1 represents another class\n",
    "labels = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 , 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # Replace '...' with labels for each video\n",
    "\n",
    "# COMMENT_SR: perhaps better to have it this way\n",
    "labels = [0 for _ in range(48)] + [1 for _ in range(8)]\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "frames_s, labels_s = shuffle(frames, labels)\n",
    "\n",
    "# Split the frames and labels (assuming you have labels for each video)\n",
    "X_train, X_test, y_train, y_test = train_test_split(frames_s, labels_s, test_size=0.2, random_state=42)\n",
    "\n",
    "# COMMENT_SR: since model.fit requires numpy arrays\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Train your model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "068f03b6",
   "metadata": {
    "id": "068f03b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 671ms/step\n",
      "Accuracy: 0.9166666666666666\n",
      "Confusion Matrix:\n",
      " [[11  0]\n",
      " [ 1  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96        11\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.92        12\n",
      "   macro avg       0.46      0.50      0.48        12\n",
      "weighted avg       0.84      0.92      0.88        12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "classification_rep = classification_report(y_test, y_pred_binary)\n",
    "print(\"Classification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c69737",
   "metadata": {
    "id": "c6c69737"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda35d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e926c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92b2c21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
